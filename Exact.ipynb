{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandanha/Documents/My_Documents/Excel_data_reader/my_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.together import TogetherLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = TogetherLLM(model=\"META-LLAMA/LLAMA-3-70B-CHAT-HF\", \n",
    "api_key=\"87cac61702f938de91651d60977c8a2978163b011f9ba6e991f0e954b9c2c2ce\")\n",
    "Settings.llm = llm\n",
    "\n",
    "# separator = \"\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PandasExcelReader\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "base_reader = PandasExcelReader()\n",
    "base_docs = base_reader.load_data(Path(\"Tv_catalogue.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "base_index = SummaryIndex.from_documents([base_docs[0]])\n",
    "base_query_engine = base_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"What are all the different Screen Type available\"\n",
    "base_response = base_query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* PandasExcelReader *******\n",
      "Based on the provided context information, the different screen types available are:\n",
      "\n",
      "1. LED\n",
      "2. QLED\n",
      "3. OLED\n"
     ]
    }
   ],
   "source": [
    "print(\"******* PandasExcelReader *******\")\n",
    "print(str(base_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 128\n",
    "Settings.context_window = 8100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00,  7.23it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.82s/it]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=1024, chunk_overlap=128\n",
    ")\n",
    "title_extractor = TitleExtractor(nodes=5)\n",
    "qa_extractor = QuestionsAnsweredExtractor(questions=3)\n",
    "\n",
    "# assume documents are defined -> extract nodes\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[text_splitter, title_extractor, qa_extractor]\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(\n",
    "    documents=base_docs,\n",
    "    in_place=True,\n",
    "    show_progress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19661/2182097066.py:16: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024, chunk_overlap=100, embed_model=embed_model)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nodes_no_metadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m service_context \u001b[38;5;241m=\u001b[39m ServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(llm\u001b[38;5;241m=\u001b[39mllm, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, embed_model\u001b[38;5;241m=\u001b[39membed_model)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# VectorStoreIndex will return index object\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m index2 \u001b[38;5;241m=\u001b[39m SummaryIndex\u001b[38;5;241m.\u001b[39mfrom_documents(\u001b[43mnodes_no_metadata\u001b[49m, storage_context\u001b[38;5;241m=\u001b[39mstorage_context)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nodes_no_metadata' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "import qdrant_client\n",
    "\n",
    "# Create a folder for local Qdrant Client\n",
    "client = qdrant_client.QdrantClient(path=\"qdrant_excel_tv\")\n",
    "\n",
    "# Create a folder text_collection Qdrant VectorStore\n",
    "text_store = QdrantVectorStore(client=client, collection_name=\"text_collection\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=text_store)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024, chunk_overlap=100, embed_model=embed_model)\n",
    "\n",
    "# VectorStoreIndex will return index object\n",
    "index2 = SummaryIndex.from_documents(nodes_no_metadata, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "# Initializing reranking model\n",
    "rerank = SentenceTransformerRerank(model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_text_qa_msgs = [\n",
    "# Defines the system role message for the chat interaction like guidelines and tone for the AI model.\n",
    "ChatMessage(\n",
    "    role=MessageRole.SYSTEM,\n",
    "    content=(\"\"\"\n",
    "        You are a AI Chatbot assistance to answer the question for the excel data i have given.\n",
    "        Your goal is to answer questions as accurately as possible based on the instructions and context provided.\n",
    "        After answering to the query at the end tell 'Let me Know if You Have any Queries'\n",
    "        If the context is Outside the Excel date Don't answer related to the question.\n",
    "        Just say that, 'The asked question is outside the context of the Excel data, I will answer to the questions related to the excel only'.\n",
    "        If the context is Outside the Excel Don't try to make up an answer.\n",
    "        For general questions like 'How are you?' or 'Who are you?', respond accordingly,\"\"\"\n",
    "    ),\n",
    "),\n",
    "# Defines the system user message for the chat interaction like query and passing relevent chunks to the AI model.\n",
    "ChatMessage(\n",
    "    role=MessageRole.USER,\n",
    "    content=(\n",
    "        \"Context information is below.\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"Given the context information and not prior knowledge, \"\n",
    "        \"answer the question: {query_str} in bullet points or numbered list where appropriate.\\n\"\n",
    "    ),\n",
    "),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index2.as_query_engine(similarity_top_k=3, node_postprocessors=[\n",
    "                                                            rerank], text_qa_template=text_qa_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"How can you help me\"\n",
    "response__2 = query_engine.query(user_query)\n",
    "response__2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"List the Tv brands available\"\n",
    "response__2 = query_engine.query(user_query)\n",
    "response__2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What was the maximum, minimum and average price of the TV\"\n",
    "response__2 = query_engine.query(user_query)\n",
    "response__2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Which brand has the highest number of TV models listed?\"\n",
    "response__2 = query_engine.query(user_query)\n",
    "response__2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"List all the model name in the TV Brand Panasonic\"\n",
    "response__2 = query_engine.query(user_query)\n",
    "response__2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"List all the model name in the TV Brand Sony\"\n",
    "response__2 = query_engine.query(user_query)\n",
    "response__2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the average price of the TVs listed?\"\n",
    "response__2 = query_engine.query(user_query)\n",
    "response__2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"How many rows are there in the excel\"\n",
    "response__2 = query_engine.query(user_query)\n",
    "response__2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the average price of the TVs listed?\"\n",
    "response__2 = query_engine.query(user_query)\n",
    "response__2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the average price of the TVs listed by considering all the rows?\"\n",
    "response__2 = query_engine.query(user_query)\n",
    "response__2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
